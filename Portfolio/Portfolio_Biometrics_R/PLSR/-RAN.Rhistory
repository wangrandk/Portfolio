install.packages("reshape2")
library(reshape2)
set.seed <- 123.234
y <-  1:7 + rnorm(7, sd = 0.2)
x1 <- 1:7 + rnorm(7, sd = 0.2)
x2 <- 1:7 + rnorm(7, sd = 0.2)
x3 <- 1:7 + rnorm(7, sd = 0.2)
x4 <- 1:7 + rnorm(7, sd = 0.2)
data1 <- matrix(c(x1, x2, x3, x4, y), ncol = 5, byrow = F)
res <- lm(y ~ x1 + x2 + x3 + x4)
summary(res)
res <- lm(y ~ x2 + x3 + x4)
summary(res)
res <- lm(y ~ x2 + x3)
summary(res)
res <- lm(y ~ x3)
summary(res)
pairs(matrix(c(x1, x2, x3, x4, y), ncol = 5, byrow = F),
labels = c("x1", "x2", "x3", "x4", "y"))
y  <- 1:7 + rnorm(7, sd = 0.2)
x1 <- 1:7 + rnorm(7, sd = 0.2)
x2 <- 1:7 + rnorm(7, sd = 0.2)
x3 <- 1:7 + rnorm(7, sd = 0.2)
x4 <- 1:7 + rnorm(7, sd = 0.2)
data2 <- matrix(c(x1, x2, x3, x4, y), ncol = 5, byrow = F)
res <- lm(y ~ x1 + x2 + x3 + x4)
summary(res)
res <- lm(y ~ x1 + x2 + x4)
summary(res)
res <- lm(y ~ x2 + x4)
summary(res)
res <- lm(y ~ x2)
summary(res)
pairs(matrix(c(x1, x2, x3, x4, y), ncol = 5, byrow = F),
labels = c("x1", "x2", "x3", "x4", "y"))
xmn1 <- (data1[,1] + data1[,2] + data1[,3] + data1[,4])/4
xmn2 <- (data2[,1] + data2[,2] + data2[,3] + data2[,4])/4
rm1 <- lm(data1[,5] ~ xmn1)
rm2 <- lm(data2[,5] ~ xmn2)
summary(rm1)
summary(rm2)
# Almost all variance explained in first component:
princomp(data1[,1:4])
# The loadings of the first component:
princomp(data1[,1:4])$loadings[,1]
cf1 <- summary(lm(data1[,5] ~ data1[,1] + data1[,2] + data1[,3] +
data1[,4]))$coefficients[,1]
cf <- summary(rm2)$coefficients[,1]
# Simulation of prediction:
error <- 0.2
y  <- rep(1:7, 1000) + rnorm(7000, sd = error)
x1 <- rep(1:7, 1000) + rnorm(7000, sd = error)
x2 <- rep(1:7, 1000) + rnorm(7000, sd = error)
x3 <- rep(1:7, 1000) + rnorm(7000, sd = error)
x4 <- rep(1:7, 1000) + rnorm(7000, sd = error)
yhat <- cf1[1] + matrix(c(x1, x2, x3, x4), ncol = 4,
byrow = F) %*% t(t(cf1[2:5]))
xmn <- (x1 + x2 + x3 + x4)/4
yhat2 <- cf[1] + cf[2] * xmn
yhat3 <- cf[1] + cf[2] * x3
barplot(c(sum((y-yhat)^2)/7000, sum((y-yhat2)^2)/7000, sum((y-yhat3)^2)/7000),
col = heat.colors(3), names.arg = c("Full MLR","Average","x3"),
cex.names = 1.5, main = "Average squared prediction error")
cf
data1
princomp(data1[,1:4])
data1[,1:4])$loadings[,1]
princomp(data1[,1:4])$loadings[,1]
cf1 <- summary(lm(data1[,5] ~ data1[,1] + data1[,2] + data1[,3] +
data1[,4]))$coefficients[,1]
cf1
cf <- summary(rm2)$coefficients[,1]
cf
xmn2
error <- 0.2
y  <- rep(1:7, 1000) + rnorm(7000, sd = error)
x1 <- rep(1:7, 1000) + rnorm(7000, sd = error)
x2 <- rep(1:7, 1000) + rnorm(7000, sd = error)
x3 <- rep(1:7, 1000) + rnorm(7000, sd = error)
x4 <- rep(1:7, 1000) + rnorm(7000, sd = error)
yhat <- cf1[1] + matrix(c(x1, x2, x3, x4), ncol = 4,
byrow = F) %*% t(t(cf1[2:5]))
cf1[1]
t(t(cf1[2:5]))
cf1[2:5]
rm(list=ls())
prostate <- read.table("prostatedata.txt", header = TRUE,sep = ";", dec = ",")
head(prostate)
View(prostate)
prostate$X<-as.matrix(prostate[,2:9])
prostate_TEST <- prostate[prostate$train == FALSE,]
prostate_TRAIN <- prostate[prostate$train == TRUE,]
summary(prostate_TEST)
View(prostate_TRAIN)
library(pls)
m1<-pcr(lpsa~X,ncomp=8,data=prostate_TRAIN,validation="LOO",scale=T,jackknife=T)
m1<-pcr(lpsa~X,ncomp=8,data=prostate_TRAIN,validation="LOO",scale=T,jackknife=T)
par(mfrow=c(2,2))
plot(m1,labels = rownames(prostate_TRAIN),which="validation")
m1<-pcr(lpsa~X,ncomp=8,data=prostate_TRAIN,validation="LOO",scale=T,jackknife=T)
par(mfrow=c(2,2))
plot(m1,labels = rownames(prostate_TRAIN),which="validation")
plot(m1,"validation",estimate=c("train","CV"),val.type ="MSEP",legendpos="topright")
plot(m1,"validation",estimate=c("train","CV"),val.type ="R2",legendpos="bottomright")
scoreplot(m1, labels = rownames(prostate_TRAIN))
loadingplot(m1,comps=8,scatter=T,labels = names(prostate_TRAIN))
install.packages("MVR")
library(pls)
m1<-pcr(lpsa~X,ncomp=8,data=prostate_TRAIN,validation="LOO",scale=T,jackknife=T)
par(mfrow=c(2,2))
plot(m1,labels = rownames(prostate_TRAIN),which="validation")
plot(m1,"validation",estimate=c("train","CV"),val.type ="MSEP",legendpos="topright")
plot(m1,"validation",estimate=c("train","CV"),val.type ="R2",legendpos="bottomright")
scoreplot(m1, labels = rownames(prostate_TRAIN))
loadingplot(m1,comps=8,scatter=T,labels = names(prostate_TRAIN))
install.packages("SCORER2")
segCV <- pcr(lpsa~X , ncomp = 8, data = prostate_TRAIN, scale = TRUE,validation = "CV", segments = 6, segment.type = "consecutive",jackknife = TRUE)
summary(segCV)
summary(segCV)
scoreplot(segCV, comps = 1:3, labels = rownames(prostate_TRAIN))
#Loadings:
loadingplot(segCV,comps = 1:3, scatter = T, labels = names(prostate_TRAIN))
m2 <- pcr(lpsa~X , ncomp = 8, data = prostate_TRAIN, validation = "LOO",scale = TRUE, jackknife = TRUE)
summary(m2)
##validate using 3 component. predicted vs. residuals from the predplot function Hence these are the (CV) VALIDATED versions####
loadings1 <- pcares$vectors[,1] #[100 100]
matplot(t(spectra), type = "n", xlab = "Wavelength", ylab = "")
matlines(t(spectra))
meansp <- apply(spectra, 2, mean)
lines(1:100, meansp, lwd = 2)
spectramc<-scale(spectra,scale=F)
matplot(t(spectramc),type="n",xlab="Wavelength",ylab="")
matlines(t(spectramc))
spectramcs<-scale(spectra,scale=T,center=T)
matplot(t(spectramcs),type="n",xlab="Wavelength",ylab="")
matlines(t(spectramcs))
# Doing the PCA on the correlation matrixs with the eigen-function:
pcares <- eigen(cor(spectra))
x <- (-39:60)/10
spectra <- matrix(rep(0, 700), ncol = 100)
for (i in 1:7) spectra[i,] <- i * dnorm(x) +
i * dnorm(x) * rnorm(100, sd = 0.02)
y <- 1:7 + rnorm(7, sd = 0.2)
matplot(t(spectra), type = "n", xlab = "Wavelength", ylab = "")
matlines(t(spectra))
meansp <- apply(spectra, 2, mean)
lines(1:100, meansp, lwd = 2)
spectramc<-scale(spectra,scale=F)
matplot(t(spectramc),type="n",xlab="Wavelength",ylab="")
matlines(t(spectramc))
spectramcs<-scale(spectra,scale=T,center=T)
matplot(t(spectramcs),type="n",xlab="Wavelength",ylab="")
matlines(t(spectramcs))
# Doing the PCA on the correlation matrixs with the eigen-function:
pcares <- eigen(cor(spectra))
loadings1 <- pcares$vectors[,1] #[100 100]
loadings1
scores1 <- spectramcs%*%t(t(loadings1)) #[7 100]*[100 1]=[7 1]
scores1
pred <- scores1 %*% loadings1 #[7 100]
pred
<-apply(spectra, 2, sd)
?apply
sd
predorg <- pred * matrix(rep(stdsp, 7), byrow=T, nrow=7) +
matrix(rep(meansp, 7), nrow=7, byrow=T)
par(mfrow = c(3, 3), mar = 0.6 * c(5, 4, 4, 2))
matplot(t(spectra), type = "n", xlab = "Wavelength",
ylab = "", main = "Raw spectra", las = 1)
matlines(t(spectra))
matplot(t(spectramc), type = "n", xlab = "Wavelength",
ylab = "", main = "Mean corrected spectra", las = 1)
matlines(t(spectramc))
matplot(t(spectramcs), type = "n", xlab = "Wavelength",
ylab = "", main = "Standardized spectra", las = 1)
matlines(t(spectramcs))
matplot(t(spectra), type = "n", xlab = "Wavelength",
ylab = "", main = "Mean Spectrum", las = 1)
lines(1:100, meansp, lwd = 2)
plot(1:100, -loadings1, ylim = c(0, 0.2), xlab = "Wavelength",
ylab = "", main = "PC1 Loadings", las = 1)
matplot(t(pred), type = "n", xlab = "Wavelength",
ylab = "", main = "Reconstruction using PC1", las = 1)
matlines(t(pred))
matplot(t(spectra), type = "n", xlab = "Wavelength",
ylab = "", main = "Standard deviations", las = 1)
lines(1:100, stdsp, lwd = 2)
plot(1:7, scores1[7:1], main = "PC1 Scores", xlab = "Samples",
ylab = "", las = 1)
matplot(t(predorg), type = "n", xlab = "Wavelength",
ylab = "", main = "Reconstruction using PC1")
matlines(t(predorg))
par(mfrow = c(1, 1))
barplot(c(10, 5, 3, 3.1, 3.2, 4, 6, 9), names.arg = 0:7,
xlab = "No components", ylab = "RMSEP", cex.names = 2,
main = "Validation results")
# Example: using Salt data:
saltdata <- read.table("LESLIE_SALT.csv", header = TRUE, sep = ";", dec = ".")
saltdata$logPrice <- log(saltdata$Price)
# Define the X-matrix as a matrix in the data frame:
saltdata$X <- as.matrix(saltdata[, 2:8])
# First of all we consider a random selection of 4 properties  as a TEST set
saltdata$train <- TRUE
saltdata$train[sample(1:length(saltdata$train), 4)] <- FALSE
saltdata_TEST <- saltdata[saltdata$train == FALSE,]
saltdata_TRAIN <- saltdata[saltdata$train == TRUE,]
# Run the PCR with maximal/large number of components using pls package:
library(pls)
mod <- pcr(logPrice ~ X , ncomp = 7, data = saltdata_TRAIN,
validation="LOO", scale = TRUE, jackknife = TRUE)
# Initial set of plots:
par(mfrow = c(2, 2))
plot(mod, labels = rownames(saltdata_TRAIN), which = "validation")
plot(mod, "validation", estimate = c("train", "CV"), legendpos = "topright")
plot(mod, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
scoreplot(mod, labels = rownames(saltdata_TRAIN))
# Choice of components:
# what would segmented CV give:
mod_segCV <- pcr(logPrice ~ X , ncomp = 7, data = saltdata_TRAIN, scale = TRUE,
validation = "CV", segments = 5, segment.type = c("random"),
jackknife = TRUE)
# Initial set of plots:
plot(mod_segCV, "validation", estimate = c("train", "CV"), legendpos = "topright")
plot(mod_segCV, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
# Let us look at some more components:
# Scores:
scoreplot(mod, comps = 1:4, labels = rownames(saltdata_TRAIN))
#Loadings:
loadingplot(mod,comps = 1:4, scatter = TRUE, labels = names(saltdata_TRAIN))
# We choose 4 components
mod4 <- pcr(logPrice ~ X , ncomp = 4, data = saltdata_TRAIN, validation = "LOO",
scale = TRUE, jackknife = TRUE)
par(mfrow = c(2, 2))
k=4
obsfit <- predplot(mod4, labels = rownames(saltdata_TRAIN), which = "validation")
Residuals <- obsfit[,1] - obsfit[,2]
plot(obsfit[,2], Residuals, type="n", main = k, xlab = "Fitted", ylab = "Residuals")
text(obsfit[,2], Residuals, labels = rownames(saltdata_TRAIN))
obsfit
plot(obsfit[,2], Residuals, type="n", main = k, xlab = "Fitted", ylab = "Residuals")
text(obsfit[,2], Residuals, labels = rownames(saltdata_TRAIN))
obsfit <- predplot(mod4, labels = rownames(saltdata_TRAIN), which = "validation")
Residuals <- obsfit[,1] - obsfit[,2]
plot(obsfit[,2], Residuals, type="n", main = k, xlab = "Fitted", ylab = "Residuals")
text(obsfit[,2], Residuals, labels = rownames(saltdata_TRAIN))
qqnorm(Residuals)
# To plot residuals against X-leverage, we need to find the X-leverage:
# AND then find the leverage-values as diagonals of the Hat-matrix:
# Based on fitted X-values:
Xf <- scores(mod4)
H <- Xf %*% solve(t(Xf) %*% Xf) %*% t(Xf)
leverage <- diag(H)
plot(leverage, abs(Residuals), type = "n", main = k)
text(leverage, abs(Residuals), labels = rownames(saltdata_TRAIN))
# Let's also plot the residuals versus each input X:
par(mfrow=c(3,3))
for ( i in 2:8){
plot(Residuals~saltdata_TRAIN[,i],type="n",xlab=names(saltdata_TRAIN)[i])
text(saltdata_TRAIN[,i],Residuals,labels=row.names(saltdata_TRAIN))
lines(lowess(saltdata_TRAIN[,i],Residuals),col="blue")
}
scores(mod4)
Xf
plot(leverage, abs(Residuals), type = "n", main = k)
text(leverage, abs(Residuals), labels = rownames(saltdata_TRAIN))
# Let's also plot the residuals versus each input X:
par(mfrow=c(3,3))
for ( i in 2:8){
plot(Residuals~saltdata_TRAIN[,i],type="n",xlab=names(saltdata_TRAIN)[i])
text(saltdata_TRAIN[,i],Residuals,labels=row.names(saltdata_TRAIN))
lines(lowess(saltdata_TRAIN[,i],Residuals),col="blue")
}
# Now let's look at the results  -  4) "interpret/conclude"
par(mfrow = c(2, 2))
# Plot coefficients with uncertainty from Jacknife:
obsfit <- predplot(mod4, labels = rownames(saltdata_TRAIN), which = "validation")
abline(lm(obsfit[,2] ~ obsfit[,1]))
plot(mod, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
coefplot(mod4, se.whiskers = TRUE, labels = prednames(mod4), cex.axis = 0.5)
biplot(mod4)
par(mfrow = c(2, 2))
# Plot coefficients with uncertainty from Jacknife:
obsfit <- predplot(mod4, labels = rownames(saltdata_TRAIN), which = "validation")
abline(lm(obsfit[,2] ~ obsfit[,1]))
plot(mod, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
coefplot(mod4, se.whiskers = TRUE, labels = prednames(mod4), cex.axis = 0.5)
biplot(mod4)
jack.test(mod4, ncomp = 4)
preds <- predict(mod4, newdata = saltdata_TEST, comps = 4)
preds
saltdata_TEST$logPrice
plot(saltdata_TEST$logPrice, preds)
rmsep <- sqrt(mean((saltdata_TEST$logPrice - preds)^2))
rmsep
mod4
rm(list=ls())
pectin <- read.table("Pectin_and_DE.txt", header = TRUE,sep = ";", dec = ",")
setwd("C:/Users/RAN/OneDrive/Documents/BioDataAnalysis/Data/PLSR")
rm(list=ls())
pectin <- read.table("Pectin_and_DE.txt", header = TRUE,sep = ";", dec = ",")
dim(pectin) #70x1040
pectin$X<-as.matrix(pectin[,2:1040])
n=ncol(pectin$X)
Mpcr<-pcr(DE~X,ncomp =60,data=pectin,validation="LOO",scale=T,jackknife=T )
par(mfrow=c(2,2))
plot(Mpcr,labels=rownames(pectin),which="validation")
plot(Mpcr,"validation",estimate=c("train","CV"),legendpos="topright")
plot(Mpcr,"validation",estimate=c("train","CV"),val.type="R2",legendpos="bottomright")
pls::scoreplot(Mpcr,labels=rownames(pectin))
pls::loadplot(Mpcr,labels=colnames(pectin))
pls::loadsplot(Mpcr,labels=colnames(pectin))
setwd("C:/Users/RAN/OneDrive/Documents/BioDataAnalysis/Data/PCR")
rm(list=ls())
prostate <- read.table("prostatedata.txt", header = TRUE,sep = ";", dec = ",")
prostate$X<-as.matrix(prostate[,2:9])
prostate_TEST <- prostate[prostate$train == FALSE,]
prostate_TRAIN <- prostate[prostate$train == TRUE,]
library(pls)
m1<-pcr(lpsa~X,ncomp=8,data=prostate_TRAIN,validation="LOO",scale=T,jackknife=T)
par(mfrow=c(2,2))
plot(m1,labels = rownames(prostate_TRAIN),which="validation")
plot(m1,"validation",estimate=c("train","CV"),val.type ="MSEP",legendpos="topright")
plot(m1,"validation",estimate=c("train","CV"),val.type ="R2",legendpos="bottomright")
scoreplot(m1, labels = rownames(prostate_TRAIN))
loadingplot(m1,comps=8,scatter=T,labels = names(prostate_TRAIN))
pls::loadingplot(m1,comps=8,scatter=T,labels = names(prostate_TRAIN))
scoreplot(m1, labels = rownames(prostate_TRAIN))
pls::loadingplot(m1,comps=8,scatter=T,labels = names(prostate_TRAIN))
pls::scoreplot(m1, labels = rownames(prostate_TRAIN))
par(mfrow = c(2, 2))
k=8
fit <- predplot(m2, labels = rownames(prostate_TRAIN), which = "validation")
Residuals <- fit[,1] - fit[,2]
plot(fit[,2], Residuals, type="n", main = k, xlab = "Fitted", ylab = "Residuals")
text(fit[,2], Residuals, labels = rownames(prostate_TRAIN))
qqnorm(Residuals)
plot(m2)
# To plot residuals against X-leverage, we need to find the X-leverage:
# AND then find the leverage-values as diagonals of the Hat-matrix:
# Based on fitted X-values:
Xf <- scores(m2)
H <- Xf %*% solve(t(Xf) %*% Xf) %*% t(Xf)
leverage <- diag(H)
plot(leverage, abs(Residuals), type = "n", main = k)
text(leverage, abs(Residuals), labels = rownames(prostate_TRAIN))
#plot the residuals versus each input X:
par(mfrow=c(3,3))
for ( i in 2:9){
plot(Residuals~prostate_TRAIN[,i],type="n",xlab=names(prostate_TRAIN)[i])
text(prostate_TRAIN[,i],Residuals,labels=row.names(prostate_TRAIN))
lines(lowess(prostate_TRAIN[,i],Residuals),col="blue")
}
dim(Residuals)
# "interpret/conclude"
par(mfrow = c(2, 2))
# Plot coefficients with uncertainty from Jacknife:
fit <- predplot(m2, labels = rownames(prostate_TRAIN), which = "validation")
abline(lm(fit[,2] ~ fit[,1]))
plot(m2, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
coefplot(m2, se.whiskers = TRUE, labels = prednames(m2), cex.axis = 1.5)
##Run the PCR with maximal/large number of components using pls package:####
library(pls)
m1<-pcr(lpsa~X,ncomp=8,data=prostate_TRAIN,validation="LOO",scale=T,jackknife=T)
par(mfrow=c(2,2))
plot(m1,labels = rownames(prostate_TRAIN),which="validation")
plot(m1,"validation",estimate=c("train","CV"),val.type ="MSEP",legendpos="topright")
plot(m1,"validation",estimate=c("train","CV"),val.type ="R2",legendpos="bottomright")
pls::scoreplot(m1, labels = rownames(prostate_TRAIN))
pls::loadingplot(m1,comps=8,scatter=T,labels = names(prostate_TRAIN))
## Choice of components:####
# what would segmented CV give:
segCV <- pcr(lpsa~X , ncomp = 8, data = prostate_TRAIN, scale = TRUE,validation = "CV", segments = 6, segment.type = "consecutive",jackknife = TRUE)
summary(segCV)
# Initial set of plots:
par(mfrow=c(1,1))
plot(segCV, "validation", estimate = c("train", "CV"), legendpos = "topright")
plot(segCV, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
#score plot:
pls::scoreplot(segCV, comps = 1:3, labels = rownames(prostate_TRAIN))
#Loadings:
pls::loadingplot(segCV,comps = 1:3, scatter = T, labels = names(prostate_TRAIN))
# We choose 3 components leave one out,
m2 <- pcr(lpsa~X , ncomp = 8, data = prostate_TRAIN, validation = "LOO",scale = TRUE, jackknife = TRUE)
summary(m2)
##validate using 3 component. predicted vs. residuals from the predplot function Hence these are the (CV) VALIDATED versions####
par(mfrow = c(2, 2))
k=8
fit <- predplot(m2, labels = rownames(prostate_TRAIN), which = "validation")
Residuals <- fit[,1] - fit[,2]
plot(fit[,2], Residuals, type="n", main = k, xlab = "Fitted", ylab = "Residuals")
text(fit[,2], Residuals, labels = rownames(prostate_TRAIN))
qqnorm(Residuals)
plot(m2)
# To plot residuals against X-leverage, we need to find the X-leverage:
# AND then find the leverage-values as diagonals of the Hat-matrix:
# Based on fitted X-values:
Xf <- scores(m2)
H <- Xf %*% solve(t(Xf) %*% Xf) %*% t(Xf)
leverage <- diag(H)
plot(leverage, abs(Residuals), type = "n", main = k)
text(leverage, abs(Residuals), labels = rownames(prostate_TRAIN))
#plot the residuals versus each input X:
par(mfrow=c(3,3))
for ( i in 2:9){
plot(Residuals~prostate_TRAIN[,i],type="n",xlab=names(prostate_TRAIN)[i])
text(prostate_TRAIN[,i],Residuals,labels=row.names(prostate_TRAIN))
lines(lowess(prostate_TRAIN[,i],Residuals),col="blue")
}
dim(Residuals)
# "interpret/conclude"
par(mfrow = c(2, 2))
# Plot coefficients with uncertainty from Jacknife:
fit <- predplot(m2, labels = rownames(prostate_TRAIN), which = "validation")
abline(lm(fit[,2] ~ fit[,1]))
plot(m2, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
coefplot(m2, se.whiskers = TRUE, labels = prednames(m2), cex.axis = 1.5)
biplot(m2,var.axes = TRUE)
biplot(Mpcr,var.axes = TRUE)
pls::biplot(Mpcr,var.axes = TRUE)
pls::scoreplot(Mpcr,labels=rownames(pectin))
setwd("C:/Users/RAN/OneDrive/Documents/BioDataAnalysis/Data/PLSR")
rm(list=ls())
pectin <- read.table("Pectin_and_DE.txt", header = TRUE,sep = ";", dec = ",")
pectin$X<-as.matrix(pectin[,2:1040])
n=ncol(pectin$X)
Mpcr<-pcr(DE~X,ncomp =60,data=pectin,validation="LOO",scale=T,jackknife=T )
par(mfrow=c(2,2))
plot(Mpcr,labels=rownames(pectin),which="validation")
plot(Mpcr,"validation",estimate=c("train","CV"),legendpos="topright")
plot(Mpcr,"validation",estimate=c("train","CV"),val.type="R2",legendpos="bottomright")
pls::scoreplot(Mpcr,labels=rownames(pectin))
pls::biplot(Mpcr,var.axes = TRUE)
pls::loadingplot(Mpcr,labels=colnames(pectin))
pls::loadingplot(Mpcr,comps=60,scatter=T,labels=colnames(pectin))
pls::loadingplot(Mpcr,comps=8,scatter=T,labels=colnames(pectin))
pls::loadingplot(Mpcr,comps=5,scatter=T,labels=names(pectin))
pls::scoreplot(Mpcr,labels=rownames(pectin))
pls::loadingplot(Mpcr,comps=5,labels=names(pectin))
Mpls<-plsr(DE~X,ncomp=60,data=pectin,validation="LOO",scale=T,jackknife=T)
par(mfrow=c(2,2))
plot(Mpls,labels=rownames(pectin),which="validation")
plot(Mpls,"validation",estimate=c("train","CV"),legendpos="topright")
plot(Mpls,"validation",estimate=c("train","CV"),val.type="R2",legendpos="bottomright")
pls::scoreplot(Mpls,labels=rownames(pectin))
par(mfrow=c(2, 2))
coefplot(Mpcr, ncomp = 1, se.whiskers = TRUE, labels = prednames(Mpcr),
cex.axis =0.5, main = "DE,PCR with 1")
coefplot(Mpcr, ncomp = 2, se.whiskers = TRUE, labels = prednames(Mpcr),
cex.axis = 0.5, main = "DE,PCR with 2")
coefplot(Mpcr, ncomp = 15, se.whiskers = TRUE, labels = prednames(Mpcr),
cex.axis = 0.5, main = "DE,PCR with 3")
coefplot(Mpls, ncomp = 10, se.whiskers = TRUE, labels = prednames(Mpls),
cex.axis = 0.5, main = "DE,PLS with 2")
coefplot(Mpcr, ncomp = 15, se.whiskers = TRUE, labels = prednames(Mpcr),
cex.axis = 0.5, main = "DE,PCR with 15")
coefplot(Mpls, ncomp = 10, se.whiskers = TRUE, labels = prednames(Mpls),
cex.axis = 0.5, main = "DE,PLS with 10")
coefplot(Mpls, ncomp = 60, se.whiskers = TRUE, labels = prednames(Mpls),
cex.axis = 0.5, main = "DE,PLS with 10")
Seg_pls<-plsr(DE~X,ncomp=60,data=pectin,validation="CV",segments=10,segment.type=c("random"),scale=T,jackknife=T)
par(mfrow=c(2,2))
plot(Seg_pls,labels=rownames(pectin),which="validation")
plot(Seg_pls,"validation",estimate=c("train","CV"),legendpos="topright")
plot(Seg_pls,"validation",estimate=c("train","CV"),val.type="R2",legendpos="bottomright")
pls::scoreplot(Seg_pls,comp=1:3,labels=rownames(pectin))
par(mfrow=c(3,3))
pls::loadingplot(Seg_pls, comps = 1, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 2, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 3, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 4, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 5, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 6, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 7, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 8, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 9, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 100, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 50, scatter = F, labels = prednames(Seg_pls),)
pls::loadingplot(Seg_pls, comps = 8, scatter = F, labels = prednames(Seg_pls),)
Mpls3<-plsr(DE~X,ncomp=8,data=pectin,validation="LOO",scale=TRUE,jackknife=TRUE)
par(mfrow=c(2, 2))
obsfit<-predplot(Mpls3,labels=rownames(pectin),which="validation")
Residuals<-obsfit[,1]-obsfit[,2]
abline(lm(obsfit[,2]~obsfit[,1]),col=2)
Mpls3
obsfit
plot(obsfit[,2], abs(Residuals), type = "n", xlab = "Fitted", ylab = "Residuals")
text(obsfit[,2], Residuals, labels = rownames(pectin))
qqnorm(Residuals)
Xf <- scores(Mpls3)
H <- Xf %*% solve(t(Xf) %*% Xf) %*% t(Xf)
leverage <- diag(H)
plot(leverage, abs(Residuals), type = "n", main = "3")
text(leverage, abs(Residuals), labels = rownames(pectin))
par(mfrow=c(2, 2))
obsfit<-predplot(Mpls3,labels=rownames(pectin),which="validation")
abline(lm(obsfit[,2]~obsfit[,1]),col=2)
plot(Mpls3, "validation", estimate = c("train", "CV"), val.type = "R2",
legendpos = "bottomright")
coefplot(Mpls3, se.whiskers = TRUE, labels = prednames(Mpls3), cex.axis = 0.5)
biplot(Mpls3,var.axes = TRUE)
##5) predict the 4 data points from the TEST set:####
preds <- predict(Mpls3, newdata =pectin, ncomp = 8)
plot(pectin$DE, preds)
rmsep<-sqrt(mean(pectin$DE-preds)^2)
rmsep
